{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samalpartha/Alzeimers/blob/main/NeuroLens_Explainable_MRI_AI_for_Alzheimer's_Staging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "134a4fc6"
      },
      "source": [
        "# NeuroLens: Explainable MRI AI for Alzheimer's Staging\n",
        "\n",
        "## Project Overview\n",
        "Deep learning system that classifies Alzheimer's stages from brain MRI scans using the Hack4Health dataset. This project fine-tunes a 2D/3D CNN with substantial data augmentation and applies Grad-CAM for explainability and comprehensive error analysis.\n",
        "\n",
        "## Key Features\n",
        "- Multi-stage Alzheimer's classification from MRI scans\n",
        "- Advanced data augmentation pipeline\n",
        "- 2D/3D CNN architecture\n",
        "- Grad-CAM visualization for model interpretability\n",
        "- Comprehensive error analysis and performance metrics\n",
        "\n",
        "## Dataset\n",
        "**Alzheimer MRI Disease Classification Dataset**\n",
        "- Path: `/kaggle/input/alzheimer-mri-disease-classification-dataset`\n",
        "- File: `train-00000-of-00001-c08a401c53fe5312.parquet`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup & Dependencies"
      ],
      "metadata": {
        "id": "mePIrUAqkW8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q pandas pillow kagglehub ipywidgets numpy matplotlib pyarrow tensorflow scikit-learn seaborn opencv-python\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Environment setup complete!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "metadata": {
        "id": "ylGLc9VekoEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "import pyarrow.parquet as pq\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import cv2\n",
        "\n",
        "# TensorFlow imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.applications import ResNet50, VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"All imports successful!\")"
      ],
      "metadata": {
        "id": "PtAX6R1CkvJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf5348cb"
      },
      "source": [
        "## 2. Data Loading & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t-UbuFCIqJ2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle API Authentication - Direct Setup\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Your Kaggle credentials\n",
        "kaggle_username = \"parthasamal\"\n",
        "kaggle_key = \"KGAT_fde7ef08afce4428cb35c261b9c6f2e1\"\n",
        "\n",
        "# Create .kaggle directory\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# Create kaggle.json with credentials\n",
        "kaggle_config = {\n",
        "    \"username\": kaggle_username,\n",
        "    \"key\": kaggle_key\n",
        "}\n",
        "\n",
        "# Write kaggle.json file\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as f:\n",
        "    json.dump(kaggle_config, f)\n",
        "\n",
        "# Set proper permissions\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "print(\"✅ Kaggle authentication configured successfully!\")\n",
        "print(f\"Username: {kaggle_username}\")\n",
        "print(\"You can now download datasets from Kaggle.\")"
      ],
      "metadata": {
        "id": "MAVx5g02p_sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset structure\n",
        "import os\n",
        "print(\"Dataset directory structure:\")\n",
        "for root, dirs, files in os.walk(path):\n",
        "    level = root.replace(path, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:10]:  # Show first 10 files\n",
        "        print(f'{subindent}{file}')\n",
        "    if len(files) > 10:\n",
        "        print(f'{subindent}... and {len(files) - 10} more files')"
      ],
      "metadata": {
        "id": "T4q8SxjPvjCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Kaggle dataset\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"borhanitrash/alzheimer-mri-disease-classification-dataset\")\n",
        "print(f\"Dataset path: {path}\")\n",
        "\n",
        "# Load the parquet file\n",
        "parquet_file = os.path.join(path, \"Alzheimer MRI Disease Classification Dataset\", \"Data\", \"train-00000-of-00001-c08a401c53fe5312.parquet\")\n",
        "df = pd.read_parquet(parquet_file)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LM5VAk1GlKzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fea9671"
      },
      "source": [
        "### Steps to check Kaggle Dataset Permissions:\n",
        "\n",
        "1.  **Go to the Dataset Page**: Open your web browser and navigate to the Kaggle dataset page: `https://www.kaggle.com/datasets/borhanitrash/alzheimer-mri-disease-classification-dataset`\n",
        "2.  **Login to Kaggle**: Ensure you are logged into Kaggle with the same account associated with the API key you are using.\n",
        "3.  **Check for 'Accept Rules' or 'Download' Button**: Look for any prompts to 'Accept Rules', 'Agree to License', or a prominent 'Download' button. Clicking these usually signifies you've accepted the terms.\n",
        "4.  **Verify Dataset Visibility**: Check if the dataset is public or private. If it's private, you might need explicit access from the dataset owner. If it's public, accepting the terms is usually sufficient.\n",
        "5.  **Review your Kaggle API Key**: Ensure your Kaggle API key is correct and up-to-date. Sometimes regenerating the key from your Kaggle account settings (`https://www.kaggle.com/me/account`) can resolve issues.\n",
        "\n",
        "Once you have verified these steps, please try running the data loading cell again."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the data\n",
        "print(\"Label distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(\"\\nLabel mapping:\")\n",
        "label_mapping = {i: label for i, label in enumerate(df['label'].unique())}\n",
        "print(label_mapping)\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, x='label')\n",
        "plt.title('Distribution of Alzheimer\\'s Disease Stages')\n",
        "plt.xlabel('Stage')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "num_classes = df['label'].nunique()\n",
        "print(f\"\\nNumber of classes: {num_classes}\")"
      ],
      "metadata": {
        "id": "c1se7qnolSqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert bytes to images and prepare dataset\n",
        "IMG_SIZE = 128  # Target image size\n",
        "\n",
        "def bytes_to_image(bytes_data):\n",
        "    \"\"\"Convert bytes to numpy array\"\"\"\n",
        "    img = Image.open(io.BytesIO(bytes_data['bytes']))\n",
        "    img = img.convert('L')  # Convert to grayscale\n",
        "    img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "    return np.array(img) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "# Prepare images and labels\n",
        "print(\"Converting images...\")\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    img_array = bytes_to_image(row['image'])\n",
        "    images.append(img_array)\n",
        "    labels.append(row['label'])\n",
        "    if (idx + 1) % 500 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(df)} images\")\n",
        "\n",
        "X = np.array(images)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Expand dimensions for CNN input (add channel dimension)\n",
        "X = np.expand_dims(X, axis=-1)\n",
        "\n",
        "print(f\"\\nX shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"Unique labels: {np.unique(y)}\")"
      ],
      "metadata": {
        "id": "NrjOif6BlvVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train, validation, and test sets\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels to integers\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "print(f\"Label classes: {le.classes_}\")\n",
        "\n",
        "# Split: 70% train, 15% validation, 15% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y_encoded, test_size=0.3, random_state=SEED, stratify=y_encoded)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=SEED, stratify=y_temp)\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
      ],
      "metadata": {
        "id": "We-sV-yzl4js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Scz1YPasl88i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build 2D CNN Model for Alzheimer's Classification\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        # First Conv Block\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Second Conv Block\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Third Conv Block\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Flatten and Dense Layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "input_shape = (IMG_SIZE, IMG_SIZE, 1)\n",
        "model = build_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ZVHQXhCRmYrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SusByUlCmgae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation for training set\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Setup callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "callbacks = [early_stop, reduce_lr, checkpoint]\n",
        "\n",
        "# Train model\n",
        "print(\"Starting training...\")\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrBCGhl6miON",
        "outputId": "4f0c48ed-b67f-40bc-9680-90bcb63eacee"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.3069 - loss: 2.0471\n",
            "Epoch 1: val_accuracy improved from -inf to 0.34766, saving model to best_model.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 6s/step - accuracy: 0.3074 - loss: 2.0448 - val_accuracy: 0.3477 - val_loss: 1.8473 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.4508 - loss: 1.4227\n",
            "Epoch 2: val_accuracy improved from 0.34766 to 0.50521, saving model to best_model.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 5s/step - accuracy: 0.4509 - loss: 1.4220 - val_accuracy: 0.5052 - val_loss: 1.5388 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.4730 - loss: 1.1773\n",
            "Epoch 3: val_accuracy did not improve from 0.50521\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 5s/step - accuracy: 0.4731 - loss: 1.1771 - val_accuracy: 0.5013 - val_loss: 2.3312 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.5086 - loss: 1.0585\n",
            "Epoch 4: val_accuracy did not improve from 0.50521\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 5s/step - accuracy: 0.5087 - loss: 1.0584 - val_accuracy: 0.5052 - val_loss: 1.2366 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.5355 - loss: 0.9914\n",
            "Epoch 5: val_accuracy improved from 0.50521 to 0.50781, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 5s/step - accuracy: 0.5354 - loss: 0.9915 - val_accuracy: 0.5078 - val_loss: 1.6046 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.5270 - loss: 0.9607\n",
            "Epoch 6: val_accuracy did not improve from 0.50781\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m595s\u001b[0m 5s/step - accuracy: 0.5271 - loss: 0.9606 - val_accuracy: 0.5078 - val_loss: 3.8304 - learning_rate: 0.0010\n",
            "Epoch 7/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "ax1.set_title('Model Accuracy Over Epochs')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Loss plot\n",
        "ax2.plot(history.history['loss'], label='Train Loss')\n",
        "ax2.plot(history.history['val_loss'], label='Val Loss')\n",
        "ax2.set_title('Model Loss Over Epochs')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DfAFqCknmp4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "49lHHpermtiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM Implementation\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    \"\"\"\n",
        "    Generate Grad-CAM heatmap for explainability\n",
        "    \"\"\"\n",
        "    # Create a model that maps input to activations and output\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    # Compute gradients\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "\n",
        "    # Pooled gradients\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # Weight feature maps by gradients\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # Normalize heatmap\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def display_gradcam(img, heatmap, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Overlay Grad-CAM heatmap on original image\n",
        "    \"\"\"\n",
        "    # Resize heatmap to match image size\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "\n",
        "    # Convert heatmap to RGB\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    # Convert grayscale to RGB\n",
        "    if len(img.shape) == 2:\n",
        "        img = np.stack([img]*3, axis=-1)\n",
        "\n",
        "    # Superimpose heatmap\n",
        "    img = np.uint8(255 * img)\n",
        "    superimposed = cv2.addWeighted(img, 1-alpha, heatmap, alpha, 0)\n",
        "\n",
        "    return superimposed\n",
        "\n",
        "print(\"Grad-CAM functions ready!\")"
      ],
      "metadata": {
        "id": "kmaKgrZ9mu8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WYwqTkh0m3wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"\\nPer-Class Accuracy:\")\n",
        "for i, class_name in enumerate(le.classes_):\n",
        "    class_mask = y_test == i\n",
        "    class_acc = accuracy_score(y_test[class_mask], y_pred[class_mask])\n",
        "    print(f\"{class_name}: {class_acc:.4f} ({np.sum(class_mask)} samples)\")"
      ],
      "metadata": {
        "id": "lXvxql36m5pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample predictions with Grad-CAM\n",
        "print(\"Generating Grad-CAM visualizations...\\n\")\n",
        "\n",
        "# Select random samples from test set\n",
        "num_samples = 6\n",
        "random_indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
        "\n",
        "# Get last convolutional layer name\n",
        "last_conv_layer = None\n",
        "for layer in reversed(model.layers):\n",
        "    if 'conv' in layer.name:\n",
        "        last_conv_layer = layer.name\n",
        "        break\n",
        "\n",
        "print(f\"Using layer: {last_conv_layer} for Grad-CAM\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(2, num_samples, figsize=(18, 6))\n",
        "\n",
        "for idx, test_idx in enumerate(random_indices):\n",
        "    img = X_test[test_idx]\n",
        "    true_label = le.classes_[y_test[test_idx]]\n",
        "\n",
        "    # Make prediction\n",
        "    img_array = np.expand_dims(img, axis=0)\n",
        "    pred_probs = model.predict(img_array, verbose=0)\n",
        "    pred_class = np.argmax(pred_probs[0])\n",
        "    pred_label = le.classes_[pred_class]\n",
        "    confidence = pred_probs[0][pred_class]\n",
        "\n",
        "    # Generate Grad-CAM\n",
        "    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer)\n",
        "\n",
        "    # Display original image\n",
        "    axes[0, idx].imshow(img.squeeze(), cmap='gray')\n",
        "    axes[0, idx].set_title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}')\n",
        "    axes[0, idx].axis('off')\n",
        "\n",
        "    # Display Grad-CAM overlay\n",
        "    gradcam_img = display_gradcam(img.squeeze(), heatmap)\n",
        "    axes[1, idx].imshow(gradcam_img)\n",
        "    axes[1, idx].set_title('Grad-CAM')\n",
        "    axes[1, idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== NeuroLens Project Complete ===\")\n",
        "print(\"All sections implemented successfully!\")"
      ],
      "metadata": {
        "id": "hOVzWAmxnCjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d1c413c"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the file path in the existing code cell to load the specified parquet file and execute the cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480731e1"
      },
      "source": [
        "## Inspect the dataframe\n",
        "\n",
        "### Subtask:\n",
        "Save to pandas df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "172c5bea"
      },
      "source": [
        "**Reasoning**:\n",
        "Display the column names, data types, and first few rows of the DataFrame to understand its structure and the nature of the image data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fa8d7cf"
      },
      "source": [
        "## Extract and display images\n",
        "\n",
        "### Subtask:\n",
        "Based on the DataFrame structure, write code to extract the image data (e.g., if it's in a specific column) and display the images. This might require using libraries like Pillow or OpenCV.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be16a1bb"
      },
      "source": [
        "**Reasoning**:\n",
        "Import PIL and iterate through the first few rows of the dataframe to extract and display the images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure pyarrow is installed in this kernel\n",
        "!pip install --quiet pyarrow\n",
        "!pip install pandas Pillow kagglehub ipywidgets numpy matplotlib pyarrow\n"
      ],
      "metadata": {
        "id": "CQTKusEUr7g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "3THkwA4LtNwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d5209d3"
      },
      "source": [
        "# Load parquet files\n",
        "# Make sure to reference download instructions. You have to download the kaggle dataset and upload it, rename it to a data directory. Video guide coming soon\n",
        "#from: https://drive.google.com/drive/folders/12-1XR8df-rYkwJuMqMgXYQ9dm6EoDx0f?usp=drive_link\n",
        "#quickstart guide: https://www.youtube.com/watch?v=WPRarAeelAM\n",
        "#Original source + description: https://advp.niagads.org/downloads\n",
        "train_df = pd.read_parquet(\"train.parquet\")\n",
        "test_df  = pd.read_parquet(\"test.parquet\")\n",
        "\n",
        "def bytes_to_pixels(b: bytes) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert raw image bytes (e.g. JPEG/PNG) into a 2D numpy array of pixel values (grayscale).\n",
        "    \"\"\"\n",
        "    img = Image.open(io.BytesIO(b))  # convert to grayscale\n",
        "    return np.array(img)\n",
        "def extract_bytes(blob):\n",
        "    \"\"\"\n",
        "    Unwrap a dict‐wrapped binary payload if needed,\n",
        "    otherwise return blob directly.\n",
        "    \"\"\"\n",
        "    if isinstance(blob, dict):\n",
        "        # try common keys\n",
        "        for key in (\"bytes\", \"data\", \"image\"):\n",
        "            if key in blob and isinstance(blob[key], (bytes, bytearray)):\n",
        "                return blob[key]\n",
        "        # fallback: first bytes‐like value\n",
        "        for v in blob.values():\n",
        "            if isinstance(v, (bytes, bytearray)):\n",
        "                return v\n",
        "        raise TypeError(f\"No bytes found in dict payload: {list(blob.keys())}\")\n",
        "    return blob\n",
        "\n",
        "train_df[\"image\"] = train_df[\"image\"].apply(lambda blob: bytes_to_pixels(extract_bytes(blob)))\n",
        "test_df[\"image\"]  = test_df[\"image\"].apply(lambda blob: bytes_to_pixels(extract_bytes(blob)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(df, n=10):\n",
        "    \"\"\"\n",
        "    Display the first n grayscale images (numpy arrays) from df\n",
        "    along with their labels. Assumes df has columns 'image' and 'label'.\n",
        "    \"\"\"\n",
        "    for i, (_, row) in enumerate(df.head(n).iterrows(), start=1):\n",
        "        pixels = row[\"image\"]\n",
        "        label = row.get(\"label\", \"\")\n",
        "        # Create a PIL image in 'L' mode (8-bit pixels, black and white)\n",
        "        img = Image.fromarray(pixels.astype(\"uint8\"), mode=\"L\")\n",
        "        print(f\"{i}. label = {label}\")\n",
        "        display(img)\n",
        "display_images(train_df)"
      ],
      "metadata": {
        "id": "WshUFiGstMEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A-OB2g7xIRKr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}